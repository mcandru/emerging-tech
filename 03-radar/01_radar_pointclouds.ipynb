{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reading and using radar pointclouds\n",
        "\n",
        "In this notebook, we will investigate some sequences from the [RadarScenes](https://radar-scenes.com/) dataset."
      ],
      "metadata": {
        "id": "U7n4yKxFN5kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gdown for downloading the dataset and h5py for reading pointclouds\n",
        "!pip install -q gdown\n",
        "!pip install -q h5py"
      ],
      "metadata": {
        "id": "6P1HMu2XM0St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Download dataset\n",
        "\n",
        "There are two sequences hosted on google drive from the RadarScenes dataset. The entire dataset is 11.1 GB, so we will look at a subset of the dataset.\n",
        "* **sequence_106**: Clear day, dual-carriageway in faster traffic\n",
        "* **sequence_87**: Rainy day, following a bus in slow traffic"
      ],
      "metadata": {
        "id": "PA5gocWiPPk-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S31QBzerOBrF"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "file_ids = {\n",
        "    \"sequence_106.zip\": \"1j03uxCt7UH8pts-aNGnkDnOcc2JaGK-6\",\n",
        "    \"sequence_87.zip\": \"1cZLWdWJrDqDXnNIlZG1lLLBFtZZCSMtb\",\n",
        "}\n",
        "\n",
        "for filename, file_id in file_ids.items():\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    gdown.download(url, f\"data/{filename}\", quiet=False)\n",
        "\n",
        "!unzip -q data/sequence_106.zip -d data/\n",
        "!unzip -q data/sequence_87.zip -d data/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Read dataset\n",
        "\n",
        "Each sequence has the following folder structure\n",
        "\n",
        "```\n",
        "sequence_X/\n",
        "├── camera\n",
        "│   ├── [timestamp_1].jpg\n",
        "│   ├── ...\n",
        "|   └── [timestamp_N].jpg\n",
        "├── radar_data.h5\n",
        "└── scenes.json\n",
        "```\n",
        "\n",
        "* Camera: Images named by the timestamp when they were recorded\n",
        "* Radar: A h5 file that contains 'odometry' and 'radar_data' datsets\n",
        "* Scenes metadata: Timestamp and sensor metadata for the sequence"
      ],
      "metadata": {
        "id": "hWD6nM9IPSy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the file structure of one of the sequences\n",
        "print(os.listdir('data/sequence_106'))"
      ],
      "metadata": {
        "id": "moClgwHdMKeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Read hdf5 dataset\n",
        "h5_path = \"data/sequence_106/radar_data.h5\"\n",
        "sequence_106 = h5py.File(h5_path, \"r\")\n",
        "\n",
        "# Read radar data into a numpy array\n",
        "pointcloud = sequence_106['radar_data'][:]\n",
        "\n",
        "# Read the number of points in the sequence\n",
        "n_points = pointcloud.size\n",
        "print(f\"n_points = {n_points}\")\n",
        "\n",
        "# Read the names of the radar data fields\n",
        "print(f\"Field names and types = {pointcloud.dtype}\")"
      ],
      "metadata": {
        "id": "lbRU1dIISrPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Visualise radar data\n",
        "\n",
        "As you can see from the previous cell, the radar data has the following fields:\n",
        "\n",
        "* **timestamp**: in micro seconds relative to some arbitrary origin\n",
        "* **sensor_id**: integer value, id of the sensor that recorded the detection\n",
        "* **range_sc**: in meters, radial distance to the detection, sensor coordinate system\n",
        "* **azimuth_sc**: in radians, azimuth angle to the detection, sensor coordinate system\n",
        "* **rcs**: in dBsm, RCS value of the detection\n",
        "* **vr**: in m/s. Radial velocity measured for this detection\n",
        "* **vr_compensated**: in m/s. Radial velocity for this detection but compensated for the ego-motion\n",
        "* **x_cc** and **y_cc**: in m, position of the detection in the car-coordinate system (origin is at the center of the rear-axle)\n",
        "* **x_seq** and **y_seq**: in m, position of the detection in the global sequence-coordinate system (origin is at arbitrary start point)\n",
        "* **uuid**: unique identifier for the detection. Can be used for association with predicted labels and debugging\n",
        "* **track_id**: id of the dynamic object this detection belongs to. Empty, if it does not belong to any.\n",
        "* **label_id**: semantic class id of the object to which this detection belongs:\n",
        "    0. passenger cars\n",
        "    1. large vehicles (like agricultural or construction vehicles)\n",
        "    2. trucks\n",
        "    3. busses\n",
        "    4. trains\n",
        "    5. bicycles (5)\n",
        "    6. motorized two-wheeler\n",
        "    7. pedestrians (7)\n",
        "    8. groups of pedestrian (8)\n",
        "    9. animals (9)\n",
        "    10. all other dynamic objects encountered while driving\n",
        "    11. static environment\n",
        "\n",
        "There are four sensors in this sequence\n",
        "1. Right\n",
        "2. Front Right\n",
        "3. Front Left\n",
        "4. Left\n",
        "\n",
        "We want to visualise the first pointcloud from sensor 1"
      ],
      "metadata": {
        "id": "VN0OylHtU2cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Split the data into separate sensors\n",
        "sensor_1 = pointcloud[pointcloud['sensor_id']==1]\n",
        "sensor_2 = pointcloud[pointcloud['sensor_id']==2]\n",
        "sensor_3 = pointcloud[pointcloud['sensor_id']==3]\n",
        "sensor_4 = pointcloud[pointcloud['sensor_id']==4]\n",
        "\n",
        "# Find unique timestamps and arange in increasing order\n",
        "timestamps_1 = np.sort(np.unique(sensor_1['timestamp']))\n",
        "timestamps_2 = np.sort(np.unique(sensor_2['timestamp']))\n",
        "timestamps_3 = np.sort(np.unique(sensor_3['timestamp']))\n",
        "timestamps_4 = np.sort(np.unique(sensor_4['timestamp']))\n",
        "\n",
        "def read_frame(timestamp_1):\n",
        "    # Read a single frame of data closest to the timestamp from sensor 1\n",
        "\n",
        "    # Find the closest timestamps from the other sensors\n",
        "    timestamp_2 = timestamps_2[np.argmin(np.abs(timestamp_1 - timestamps_2))]\n",
        "    timestamp_3 = timestamps_3[np.argmin(np.abs(timestamp_1 - timestamps_3))]\n",
        "    timestamp_4 = timestamps_4[np.argmin(np.abs(timestamp_1 - timestamps_4))]\n",
        "\n",
        "    # Select the first frame (points that match the first timestamp)\n",
        "    frame_1 = sensor_1[sensor_1['timestamp'] == timestamp_1]\n",
        "    frame_2 = sensor_2[sensor_2['timestamp'] == timestamp_2]\n",
        "    frame_3 = sensor_3[sensor_3['timestamp'] == timestamp_3]\n",
        "    frame_4 = sensor_4[sensor_4['timestamp'] == timestamp_4]\n",
        "\n",
        "    return np.concatenate((frame_1, frame_2, frame_3, frame_4))\n",
        "\n",
        "# Select frame 10\n",
        "timestamp_1 = timestamps_1[10]\n",
        "print(f\"timestamp = {timestamp_1*1e-6}\")\n",
        "\n",
        "# Read frame 10\n",
        "frame = read_frame(timestamp_1)\n",
        "print(f\"The pointcloud frame has {frame.size} points\")\n",
        "\n",
        "# Plot pointcloud frame\n",
        "max_range = np.max(pointcloud['range_sc'])\n",
        "plt.figure(figsize=(10,10))\n",
        "sc = plt.scatter(-frame['y_cc'], frame['x_cc'], c=frame['sensor_id'])\n",
        "plt.xlabel('Y (metres)')\n",
        "plt.ylabel('X (metres)')\n",
        "plt.xlim([-max_range/2, max_range/2])\n",
        "plt.ylim([0, max_range])\n",
        "plt.grid()\n",
        "\n",
        "# Create legend\n",
        "handles, _ = sc.legend_elements(prop=\"colors\", num=4)\n",
        "plt.legend(handles, ['1 - Right', '2 - Front right', '3 - Front left', '4 - Left'], title=\"Sensor\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "__CF5zYwViJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Visualise camera\n",
        "\n",
        "One tricky part of reading data from multiple sensors is time synchronisation. We have now read 4 pointclouds from 4 radars, but we want to see what that looks like from the camera. Luckily, the \"scenes.json\" file contains matching camera images for each radar timestamp so we can synchronise the radar and camera data.\n",
        "\n",
        "The scenes.json format is as follow:\n",
        "\n",
        "```\n",
        "- sequence_name\n",
        "- category\n",
        "- first_timestamp\n",
        "- last_timestamp\n",
        "- scenes\n",
        "    - [timestamp_i]\n",
        "        - sensor_id\n",
        "        - prev_timestamp\n",
        "        - next_timestamp\n",
        "        - prev_timestamp_same_sensor\n",
        "        - next_timestamp_same_sensor\n",
        "        - odometry_timestamp\n",
        "        - odometry_index\n",
        "        - image_name\n",
        "    - [timestamp_i+1]\n",
        "    ...\n",
        "```\n",
        "\n",
        "We can use the `image_name` field to find the correct camera image."
      ],
      "metadata": {
        "id": "ukOy9GXX1iqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.image as img\n",
        "\n",
        "# Read the scenes.json file\n",
        "with open('data/sequence_106/scenes.json') as f:\n",
        "    scenes = json.load(f)['scenes']\n",
        "\n",
        "# Find the corresponding timestamp\n",
        "camera_filename = scenes[str(timestamps_1[10])]['image_name']\n",
        "\n",
        "# Read camera image\n",
        "image = img.imread(f\"data/sequence_106/camera/{camera_filename}\")\n",
        "\n",
        "# Displaying the image\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fBMexWcW1hGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Segmenting pointclouds\n",
        "\n",
        "Not all points are useful for every use case. Therefore, we need to be able to extract useful points by segmenting the pointcloud. This can be done using boolean indexing in numpy arrays."
      ],
      "metadata": {
        "id": "SVfqkfcnuGuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read frame 10\n",
        "timestamp_1 = timestamps_1[10]\n",
        "frame = read_frame(timestamp_1)\n",
        "\n",
        "# Plot pointcloud frame with compensated radial velocity in m/s\n",
        "plt.figure(figsize=(10,10))\n",
        "sc = plt.scatter(-frame['y_cc'], frame['x_cc'], c=frame['vr_compensated'], vmin=-30, vmax=30)\n",
        "plt.xlabel('Y (metres)')\n",
        "plt.ylabel('X (metres)')\n",
        "plt.xlim([-max_range/2, max_range/2])\n",
        "plt.ylim([0, max_range])\n",
        "plt.grid()\n",
        "\n",
        "# Plot colorbar\n",
        "cbar = plt.colorbar(sc)\n",
        "cbar.set_label(\"Compensated Radial Velocity (m/s)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nrFLa4AevA0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only show points where the compensated velocity is greater than 2 m/s\n",
        "segmented_frame = frame[frame['vr_compensated'] > 2]\n",
        "\n",
        "# Plot pointcloud frame with compensated radial velocity in km/h\n",
        "plt.figure(figsize=(10,10))\n",
        "sc = plt.scatter(-segmented_frame['y_cc'], segmented_frame['x_cc'], c=segmented_frame['vr_compensated'], vmin=-30, vmax=30)\n",
        "plt.xlabel('Y (metres)')\n",
        "plt.ylabel('X (metres)')\n",
        "plt.xlim([-max_range/2, max_range/2])\n",
        "plt.ylim([0, max_range])\n",
        "plt.grid()\n",
        "\n",
        "# Plot colorbar\n",
        "cbar = plt.colorbar(sc)\n",
        "cbar.set_label(\"Compensated Radial Velocity (m/s)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LiilD_oH0VMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Animate pointcloud and camera"
      ],
      "metadata": {
        "id": "JFcP7bb29EoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "pfig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "scat1 = axes[0].scatter([], [])\n",
        "\n",
        "axes[0].set_xlim([-max_range/2, max_range/2])\n",
        "axes[0].set_ylim([0, max_range])\n",
        "axes[0].grid()\n",
        "\n",
        "im = axes[1].imshow(np.zeros((9, 20, 3)))\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "display_handle = display(pfig, display_id=True)\n",
        "\n",
        "try:\n",
        "    for i in range(0, timestamps_1.size, 20):\n",
        "        timestamp_1 = timestamps_1[i]\n",
        "\n",
        "        frame = read_frame(timestamp_1)\n",
        "\n",
        "        camera_filename = scenes[str(timestamp_1)]['image_name']\n",
        "        image = img.imread(f\"data/sequence_106/camera/{camera_filename}\")\n",
        "\n",
        "        # --- Update scatter data ---\n",
        "        scat1.set_offsets(np.c_[-frame['y_cc'], frame['x_cc']])\n",
        "        scat1.set_array(frame['label_id'])\n",
        "\n",
        "        im.set_data(image)\n",
        "\n",
        "        display_handle.update(pfig)\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ],
      "metadata": {
        "id": "YaKc0IsU-4Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenges\n",
        "\n",
        "1. Plot a pointcloud frame showing radial velocity (`vr` field)\n",
        "2. Print a list of all label_id classes that appear in sequence_106\n",
        "3. Show the static scene only (points less than 0.5m/s) , using the vr_compensated field\n",
        "4. At the beginning of the script, two sequences were imported, but we have only used sequence_106. Show the first pointcloud frame and camera image from the other sequence (sequence_87).\n",
        "5. Proximity detection - For timestamp 22547463510351 of sequence_87, find passenger car points inside the bounding box `y_cc` = [-5, +5], `x_cc` = [-5, +5]. Plot the detected points over the original pointcloud and plot the bounding box as well."
      ],
      "metadata": {
        "id": "q0rDUa0JBkH3"
      }
    }
  ]
}