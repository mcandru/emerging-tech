{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üí¨ Building Software on top of LLMs with Gemini\n",
        "\n",
        "Welcome! This notebook will guide you through using LLMs via API.\n",
        "\n",
        "**Before you start**, make sure you have:\n",
        "1. Your Gemini API key from [aistudio.google.com](https://aistudio.google.com)\n",
        "2. Added it to Colab Secrets (üîë icon in left sidebar) with the name `GEMINI_API_KEY`\n",
        "\n",
        "**Note:** The free tier API allowance from Gemini is quite restrictive. If you find that you have hit your limit, the open source `Gemma` models are all available via the API with higher rate limits."
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üîß Part 1: Setup\n",
        "\n",
        "Run this cell first to install the library and configure your API key."
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# Install the Google AI library (only needs to run once)\n",
        "!pip install -q google-genai\n",
        "\n",
        "# Import and configure\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Get your API key from Colab secrets\n",
        "try:\n",
        "    genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "    print(\"‚úÖ API key configured successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"Error: Could not find API key.\")\n",
        "    print(\"Make sure you've added GEMINI_API_KEY to your Colab secrets (üîë icon)\")\n",
        "    print(f\"Error details: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test - run this to make sure everything works\n",
        "model = genai.GenerativeModel('gemini-3-flash-preview')\n",
        "response = model.generate_content(\"Say hello in three different languages\")\n",
        "print(\"Test response:\")\n",
        "print(response.text)\n",
        "print(\"\\nIf you see greetings above, you're all set!\")"
      ],
      "metadata": {
        "id": "test-setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 2: Try out Gemini Models in the Playground\n",
        "\n",
        "Before calling models from the API, try out prompting different Gemini models in the AI studio playground here:\n",
        "\n",
        "https://aistudio.google.com/prompts/new_chat\n",
        "\n",
        "Some things to experiment with:\n",
        "\n",
        "- Adjusting levels of thinking\n",
        "- Adding System Instructions\n",
        "- Adjusting the temperature\n",
        "- Enabling \"Grounding with Google Search\"\n",
        "- Enabling code execution"
      ],
      "metadata": {
        "id": "part3-header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üìö Part 3: Your First API Calls\n",
        "\n",
        "Now let's explore what you can do with the API!\n",
        "\n",
        "First let's get a list of all of the different models that are available to use:"
      ],
      "metadata": {
        "id": "YHrVTYk9grR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "id": "qQpIume7xCr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.1: Basic API Call\n",
        "\n",
        "The simplest way to use the API - send a prompt, get a response."
      ],
      "metadata": {
        "id": "ex31-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-3-flash-preview')\n",
        "\n",
        "prompt = \"Explain what an API is in exactly two sentences.\"\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "ex31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR TURN: Modify the prompt above!\n",
        "# Try:\n",
        "#   - Change \"two sentences\" to \"one sentence\" or \"three bullet points\"\n",
        "#   - Change the topic from APIs to something else\n",
        "\n",
        "prompt = \"YOUR PROMPT HERE\"  # <-- Edit this!\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "ex31-try"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.2: Processing Multiple Items\n",
        "\n",
        "One of the API's superpowers: processing many items automatically!"
      ],
      "metadata": {
        "id": "ex32-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A list of tech buzzwords we want explained simply\n",
        "buzzwords = [\n",
        "    \"llm\",\n",
        "    \"vibe coding\",\n",
        "]\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "\n",
        "print(\"Tech Jargon Translator\")\n",
        "print(\"=\" * 50)\n",
        "print()\n",
        "\n",
        "for word in buzzwords:\n",
        "    prompt = f\"Explain '{word}' in one simple sentence that my grandmother would understand.\"\n",
        "    response = model.generate_content(prompt)\n",
        "    print(f\"üìå {word.upper()}\")\n",
        "    print(f\"   {response.text}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "ex32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.3: System Instructions\n",
        "\n",
        "Give the model a persistent \"personality\" or set of rules."
      ],
      "metadata": {
        "id": "ex33-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-3-flash-preview',\n",
        "    system_instruction=\"\"\"You are a dramatic movie trailer narrator.\n",
        "\n",
        "    Rules:\n",
        "    - Everything you say should sound like it's from an epic movie trailer\n",
        "    - Use dramatic pauses (written as \"...\")\n",
        "    - Keep responses short but INTENSE\n",
        "    - End with a tagline\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Now all responses will follow these rules\n",
        "response = model.generate_content(\"What is photosynthesis?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "ex33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR TURN: Create your own system prompts and see what you can do!\n",
        "\n",
        "my_model = genai.GenerativeModel(\n",
        "    'gemini-2.5-flash',\n",
        "    system_instruction=\"\"\"YOUR SYSTEM PROMPT HERE\n",
        "\n",
        "    Rules:\n",
        "    - Rule 1\n",
        "    - Rule 2\n",
        "    - Rule 3\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "response = my_model.generate_content(\"What is the weather like today?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "ex33-try"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.4: Structured Output with JSON\n",
        "\n",
        "Get data in a specific format you can use in your code."
      ],
      "metadata": {
        "id": "ex34-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "food = \"cookies\"\n",
        "\n",
        "prompt = f\"\"\"Give me a recipe for {food}\n",
        "\n",
        "Return a JSON object with these exact fields:\n",
        "{{\n",
        "    \"recipe_name\": \"...\",\n",
        "    \"ingredients\": [\"...\", \"...\", \"...\"],\n",
        "    \"instructions\": [\"...\", \"...\", \"...\"],\n",
        "}}\n",
        "\n",
        "Return ONLY the JSON, no other text.\n",
        "\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-2.5-flash',\n",
        ")\n",
        "\n",
        "response = model.generate_content(prompt, generation_config=genai.GenerationConfig(\n",
        "    response_mime_type=\"application/json\",\n",
        "))\n",
        "\n",
        "recipe = json.loads(response.text)\n",
        "\n",
        "print(\"Name:\", recipe[\"recipe_name\"])\n",
        "\n",
        "print(\"Ingredients:\")\n",
        "for ingredient in recipe[\"ingredients\"]:\n",
        "    print(f\"  - {ingredient}\")\n",
        "\n",
        "print(\"Instructions:\")\n",
        "for step in recipe[\"instructions\"]:\n",
        "    print(f\"  {step}\")"
      ],
      "metadata": {
        "id": "vYuexaBfQ7EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR TURN: Create a JSON schema to return some structured data for an LLM task\n",
        "\n",
        "prompt = f\"\"\"INSERT PROMPT HERE\n",
        "\n",
        "Return a JSON object with these exact fields:\n",
        "{{\n",
        "    \"technology_name\": \"...\",\n",
        "    \"maturity_level\": \"emerging/growing/mature\",\n",
        "    \"time_to_mainstream\": \"...\",\n",
        "    \"top_3_benefits\": [\"...\", \"...\", \"...\"],\n",
        "    \"top_3_challenges\": [\"...\", \"...\", \"...\"],\n",
        "    \"confidence_score\": 0.0 to 1.0\n",
        "}}\n",
        "\n",
        "Return ONLY the JSON, no other text.\n",
        "\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-2.5-flash',\n",
        ")\n",
        "\n",
        "response = model.generate_content(prompt, generation_config=genai.GenerationConfig(\n",
        "    response_mime_type=\"application/json\",\n",
        "))\n",
        "\n",
        "recipe = json.loads(response.text)\n",
        "print(json.dumps(recipe, indent=2))"
      ],
      "metadata": {
        "id": "ex34-try"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.6: Images via the API üñºÔ∏è\n",
        "\n",
        "You can also send images to the API! First, upload an image to Colab:\n",
        "1. Run the cell below and click \"Browse\" to upload an image from your computer"
      ],
      "metadata": {
        "id": "ex36-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "y4j7M2f2zwIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "\n",
        "filename = list(uploaded.keys())[0]\n",
        "image = PIL.Image.open(filename)\n",
        "\n",
        "# Ask about the image\n",
        "response = model.generate_content([\n",
        "    \"Describe this image in a fun, creative way.\",\n",
        "    image\n",
        "])\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "ex36-basic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different prompts with the same image!\n",
        "\n",
        "prompt = \"Write a haiku inspired by this image.\"\n",
        "\n",
        "response = model.generate_content([prompt, image])\n",
        "print(f\"üì∏ {prompt}\")\n",
        "print(f\"   {response.text}\\n\")"
      ],
      "metadata": {
        "id": "ex36-multiple-prompts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: Try out gemma-3-27b-it or some of the other available models to see how they differ in results"
      ],
      "metadata": {
        "id": "FO4hkVKbZG0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.7: Tool Use üî®\n",
        "\n",
        "Tool calling (also known as function calling) allows LLMs to interact with external systems by calling functions you define. Instead of just generating text, the model can:\n",
        "\n",
        "1. Recognise when a user's request requires external data or actions\n",
        "2. Generate a structured function call with the appropriate arguments\n",
        "3. Use the function's response to formulate its final answer\n",
        "\n",
        "Run the cell below to see how the `get_weather` tool is called. Swap out the prompts and see if the tool is still called for questions that are not about the weather."
      ],
      "metadata": {
        "id": "-EgRVNfzaSJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get the current weather for a city.\"\"\"\n",
        "    # Fake weather data for demo purposes\n",
        "    print(f\"TOOL: Calling the weather tool for {city}\")\n",
        "    weather_data = {\n",
        "        \"london\": \"Cloudy, 12¬∞C\",\n",
        "        \"paris\": \"Sunny, 18¬∞C\",\n",
        "        \"tokyo\": \"Rainy, 15¬∞C\",\n",
        "    }\n",
        "    return weather_data.get(city.lower(), f\"Weather data not available for {city}\")\n",
        "\n",
        "\n",
        "# Create the model with the tool\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-2.5-flash\",\n",
        "    tools=[get_weather],\n",
        ")\n",
        "\n",
        "chat = model.start_chat(enable_automatic_function_calling=True)\n",
        "\n",
        "response = chat.send_message(\"What's the weather like in Paris?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "qDTWDEFBaRAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üõ†Ô∏è Part 4: Build Something Useful!\n",
        "\n",
        "Build a tool to parse line items from receipts. For each line item on the receipt, parse the description and the amount.\n",
        "\n",
        "**Use the sample code above along with the Gemini assistant built into Colab or any other LLMs you wish to help you out!**\n",
        "\n",
        "When you're done, find other images of receipts on the web to test the parser out and see how well it performs."
      ],
      "metadata": {
        "id": "part4-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "import requests\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "url = \"https://www.thesun.ie/wp-content/uploads/sites/3/2023/09/dunens1.png\"\n",
        "response = requests.get(url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "display(img)"
      ],
      "metadata": {
        "id": "TV9gN8vP-jB1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}